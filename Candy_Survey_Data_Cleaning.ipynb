{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d94ed6",
   "metadata": {},
   "source": [
    "# Halloween Candy Survey - Data Cleaning and Preparation\n",
    "\n",
    "## Overview\n",
    "\n",
    "The purpose of this project was to clean and wrangle data from a survey of Halloween candy to prepare it for a machine learning project. This has been modified from a data cleaning assignment completed for my Master's program.\n",
    "\n",
    "\n",
    "## Data Set\n",
    "\n",
    "The data set used is the 2017 Halloween Candy Hierarchy data set as discussed in this [boingboing](https://boingboing.net/2017/10/30/the-2017-halloween-candy-hiera.html) article.\n",
    "\n",
    "The following are the rating instructions from the survey:  \n",
    "\n",
    "> Basically, consider that feeling you get when you receive this item in your Halloween haul. Does it make you really happy (JOY)? Or is it something that you automatically place in the junk pile (DESPAIR)? MEH for indifference, and you can leave blank if you have no idea what the item is.\n",
    "\n",
    "This dataset provided an opportunity to work with very messy data, as respondents were allowed to enter unconstrained text for a number of the fields. \n",
    "\n",
    "\n",
    "## Our End Goal\n",
    "\n",
    "The end goal for this project is to clean the data so that it could be used to create a machine learning model in the future. We want to see if we are able to predict a person's gender based purely on their candy preferences. Note: a model was not created in this project. The data was simply cleaned and prepared for use.\n",
    "\n",
    "## Initial Import & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63925b1",
   "metadata": {},
   "source": [
    "Data was first imported and a DataFrame was created called `candy`. `encoding='iso-8859-1'` was used during the import because of special characters in the some respondent answers that were not recognized by Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv with iso-8859-1 encoding\n",
    "candy_full = pd.read_csv('candy.csv', encoding='iso-8859-1')\n",
    "\n",
    "candy = candy_full.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b068f53",
   "metadata": {},
   "source": [
    "I first looked at the data column types using `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first five rows\n",
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43c4af",
   "metadata": {},
   "source": [
    "Then, looked at information about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e49431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check info about the DataFrame\n",
    "candy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125ac1f",
   "metadata": {},
   "source": [
    "Looking at column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of column names\n",
    "for col in candy.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec5c93",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b999823",
   "metadata": {},
   "source": [
    "I began cleaning by removing the character `Õ`, which is meant to be an apostrophe `'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7054e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy.rename({'Q6 | Chick-o-Sticks (we donÕt know what that is)' : \"Q6 | Chick-o-Sticks (we don't know what that is)\", \n",
    "              'Q6 | HersheyÕs Milk Chocolate' : \"Q6 | Hershey's Milk Chocolate\", \n",
    "              'Q6 | Peanut M&MÕs' : \"Q6 | Peanut M&M's\", \n",
    "              'Q6 | ReeseÕs Peanut Butter Cups' : \"Q6 | Reese's Peanut Butter Cups\"}, \n",
    "            axis = 1, inplace = True)\n",
    "\n",
    "for col in candy.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3480c08",
   "metadata": {},
   "source": [
    "I then determined how many duplicated rows there were in the file, assuming that a duplicate is any row with the same `Internal ID` number as another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91180c38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'candy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dup \u001b[38;5;241m=\u001b[39m candy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInternal ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      2\u001b[0m dup\n",
      "\u001b[1;31mNameError\u001b[0m: name 'candy' is not defined"
     ]
    }
   ],
   "source": [
    "dup = candy['Internal ID'].duplicated().sum()\n",
    "dup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c884a",
   "metadata": {},
   "source": [
    "Duplicates were then dropped from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0140f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy.drop_duplicates(['Internal ID'], inplace = True)\n",
    "print(candy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9990b5",
   "metadata": {},
   "source": [
    "The following irrelevant columns were then removed from the DataFrame:\n",
    "`Internal ID`, `Q5: STATE, PROVINCE, COUNTY, ETC`, `Q7: JOY OTHER`, `Q8: DESPAIR OTHER`, `Q9: OTHER COMMENTS`, `Unnamed: 113`, `Click Coordinates (x, y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Internal ID', 'Q5: STATE, PROVINCE, COUNTY, ETC', 'Q7: JOY OTHER', 'Q8: DESPAIR OTHER', \n",
    "             'Q9: OTHER COMMENTS', 'Unnamed: 113', 'Click Coordinates (x, y)']\n",
    "\n",
    "candy = candy.drop(drop_cols, axis = 1)\n",
    "print(candy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e2618",
   "metadata": {},
   "source": [
    "Next, the `Q2: GENDER` column was explored, as this will be the response variable in the machine learning model. Value counts and missing values are analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy['Q2: GENDER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_gender = candy['Q2: GENDER'].isnull().sum()\n",
    "missing_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab664e",
   "metadata": {},
   "source": [
    "All rows with a missing value in the `Q2: GENDER` column were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy = candy.dropna(subset = ['Q2: GENDER'])\n",
    "candy['Q2: GENDER'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc447220",
   "metadata": {},
   "source": [
    "We want to predict between Male or Female in this model. Because of this, only the rows that contain either Male or Female in the `Q2: GENDER` column were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac202ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy = candy.drop(candy[candy['Q2: GENDER'] == \"I'd rather not say\"].index)\n",
    "candy = candy.drop(candy[candy['Q2: GENDER'] == \"Other\"].index)\n",
    "candy['Q2: GENDER'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1267a0",
   "metadata": {},
   "source": [
    "Missing values in the `Q1: GOING OUT?` were then evaluated, as we want individuals who did go out on Halloween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_out = candy['Q1: GOING OUT?'].isnull().sum()\n",
    "missing_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8e86d",
   "metadata": {},
   "source": [
    "Any missing values were then filled in with a `No` response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70355d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy['Q1: GOING OUT?'] = candy['Q1: GOING OUT?'].fillna('No')\n",
    "candy['Q1: GOING OUT?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f5536",
   "metadata": {},
   "source": [
    "To get ready for the next step, I sliced a subset of columns for cleaning: `Q6 | 100 Grand Bar` to `Q11: DAY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f31921",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy_slice = candy.loc[:, 'Q6 | 100 Grand Bar':'Q11: DAY']\n",
    "for col in candy_slice.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b01af2",
   "metadata": {},
   "source": [
    "I then filled any missing values in the `candy` DataFrame for those columns with the string `NO_ANSWER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a06b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy.loc[:, 'Q6 | 100 Grand Bar':'Q11: DAY'] = candy_slice.fillna('NO_ANSWER')\n",
    "candy.info(max_cols=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e4185",
   "metadata": {},
   "source": [
    "For all four `Q12: Media` columns, I filled the missing values with `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bf4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy.loc[:, 'Q12: MEDIA [Daily Dish]':'Q12: MEDIA [Yahoo]'] = candy.loc[:, 'Q12: MEDIA [Daily Dish]':'Q12: MEDIA [Yahoo]'].fillna(0.0)\n",
    "candy.loc[:, 'Q12: MEDIA [Daily Dish]':'Q12: MEDIA [Yahoo]'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4942b4",
   "metadata": {},
   "source": [
    "The next major column to address, which is filled with very messy data, provides the countries of respondents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa655038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values\n",
    "candy['Q4: COUNTRY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ef051",
   "metadata": {},
   "source": [
    "I wanted to clean up this data to only include four areas: USA, Canada, Europe, and Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust any null values to 'Other'\n",
    "candy.loc[:, 'Q4: COUNTRY'] = candy.loc[:, 'Q4: COUNTRY'].fillna('Other')\n",
    "\n",
    "# combine Australia values into 'Other'\n",
    "candy['Q4: COUNTRY'].replace({\n",
    "    'australia' : 'Other',\n",
    "    'Australia' : 'Other'\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all 'USA' entries\n",
    "candy['Q4: COUNTRY'].replace({\n",
    "    'USA ' : 'USA',\n",
    "    'us' : 'USA',\n",
    "    'usa' : 'USA', \n",
    "    'Us' : 'USA', \n",
    "    'US' : 'USA', \n",
    "    'Murica' : 'USA', \n",
    "    'United States' : 'USA', \n",
    "    'united states' : 'USA', \n",
    "    'Usa' : 'USA', \n",
    "    'United States ' : 'USA', \n",
    "    'United staes' : 'USA', \n",
    "    'United States of America' : 'USA', \n",
    "    'United states' : 'USA', \n",
    "    'u.s.a.' : 'USA', \n",
    "    'United States of America ' : 'USA', \n",
    "    'america' : 'USA', \n",
    "    'U.S.A.' : 'USA', \n",
    "    'unhinged states' : 'USA', \n",
    "    'united states of america' : 'USA', \n",
    "    'US of A' : 'USA', \n",
    "    'The United States' : 'USA', \n",
    "    'North Carolina ' : 'USA', \n",
    "    'Unied States' : 'USA', \n",
    "    'U S' : 'USA', \n",
    "    'u.s.' : 'USA', \n",
    "    'The United States of America' : 'USA', \n",
    "    'unite states' : 'USA',\n",
    "    'U.S.' : 'USA', \n",
    "    'USA? Hard to tell anymore..' : 'USA', \n",
    "    \"'merica\" : 'USA', \n",
    "    'United State' : 'USA', \n",
    "    'United Sates' : 'USA', \n",
    "    'California' : 'USA', \n",
    "    'Unites States' : 'USA', \n",
    "    'USa' : 'USA', \n",
    "    'I pretend to be from Canada, but I am really from the United States.' : 'USA', \n",
    "    'Usa ' : 'USA', \n",
    "    'United Stated' : 'USA', \n",
    "    'New Jersey' : 'USA', \n",
    "    'United ststes' : 'USA', \n",
    "    'America' : 'USA', \n",
    "    'United Statss' : 'USA', \n",
    "    'murrika' : 'USA', \n",
    "    'USA! USA! USA!' : 'USA', \n",
    "    'USAA' : 'USA', \n",
    "    'united States ' : 'USA', \n",
    "    'N. America' : 'USA', \n",
    "    'USSA' : 'USA', \n",
    "    'U.S. ' : 'USA', \n",
    "    'u s a' : 'USA', \n",
    "    'United Statea' : 'USA', \n",
    "    'united ststes' : 'USA', \n",
    "    'USA USA USA!!!!' : 'USA'\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all Canada entries as 'CA'\n",
    "\n",
    "candy['Q4: COUNTRY'].replace({\n",
    "    'canada' : 'CA', \n",
    "    'Canada' : 'CA', \n",
    "    'canada ' : 'CA', \n",
    "    'Canada ' : 'CA', \n",
    "    'Can' : 'CA', \n",
    "    'Canae' : 'CA', \n",
    "    'Canada`' : 'CA', \n",
    "    'CANADA' : 'CA'\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all Europe entries as 'EU'\n",
    "\n",
    "candy['Q4: COUNTRY'].replace({\n",
    "   'uk' : 'EU', \n",
    "    'United Kingdom' : 'EU', \n",
    "    'England' : 'EU', \n",
    "    'UK' : 'EU', \n",
    "    'france' : 'EU', \n",
    "    'finland' : 'EU', \n",
    "    'Netherlands' : 'EU', \n",
    "    'germany' : 'EU', \n",
    "    'Europe' : 'EU', \n",
    "    'U.K. ' : 'EU', \n",
    "    'Greece' : 'EU', \n",
    "    'France' : 'EU', \n",
    "    'Ireland' : 'EU', \n",
    "    'Uk' : 'EU', \n",
    "    'Germany' : 'EU', \n",
    "    'Scotland' : 'EU', \n",
    "    'UK ' : 'EU', \n",
    "    'Denmark' : 'EU', \n",
    "    'France ' : 'EU', \n",
    "    'Switzerland' : 'EU', \n",
    "    'Scotland ' : 'EU', \n",
    "    'The Netherlands' : 'EU', \n",
    "    'Ireland ' : 'EU', \n",
    "    'spain' : 'EU', \n",
    "    'Sweden' : 'EU', \n",
    "    'United kingdom' : 'EU'\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change any remaining entries to 'Other'\n",
    "\n",
    "candy.loc[~candy['Q4: COUNTRY'].isin(['USA', 'CA', 'EU']), 'Q4: COUNTRY'] = 'Other'\n",
    "\n",
    "# code check\n",
    "candy['Q4: COUNTRY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e11c7",
   "metadata": {},
   "source": [
    "Next, the `Age` column was addressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique age values\n",
    "candy['Q3: AGE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987ed6f",
   "metadata": {},
   "source": [
    "These values were placed into the following categorical bins: `unknown`, `17 and under`, `18-25`, `26-35`, `36-45`, `46-55`, and `56+`. Any text values were binned with `unknown` for ease in this project. Categories were then reindexed in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create True/False index\n",
    "age_index = candy['Q3: AGE'].str.isnumeric()\n",
    "\n",
    "# for the index, fill missing values with False\n",
    "age_index = age_index.fillna(False)\n",
    "\n",
    "# select Age column for only those False values from index and code as missing\n",
    "candy.loc[~age_index, 'Q3: AGE'] = np.nan\n",
    "\n",
    "candy['Q3: AGE'] = candy['Q3: AGE'].astype('float')\n",
    "\n",
    "candy['Q3: AGE'] = pd.cut(x = candy['Q3: AGE'], bins = [-1, 17, 25, 35, 45, 55, 100], \n",
    "                         labels = ['17 and under', '18-25', '26-35', '36-45', '46-55', '56+'])\n",
    "\n",
    "candy['Q3: AGE'] = candy['Q3: AGE'].cat.add_categories('unknown')\n",
    "candy['Q3: AGE'] = candy['Q3: AGE'].cat.as_unordered()\n",
    "candy['Q3: AGE'] = candy['Q3: AGE'].cat.reorder_categories(\n",
    "    ['unknown', '17 and under', '18-25', '26-35', '36-45', '46-55', '56+'])\n",
    "candy.loc[:, 'Q3: AGE'] = candy.loc[:, 'Q3: AGE'].fillna('unknown')\n",
    "\n",
    "# double check categories\n",
    "candy['Q3: AGE'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b627e",
   "metadata": {},
   "source": [
    "Final checks were performed to ensure there were no missing values, and the index was reset for easier grading in the original assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value check\n",
    "candy.isnull().sum()\n",
    "\n",
    "# index reset\n",
    "candy = candy.reset_index(drop = True)\n",
    "#candy.info(max_cols=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361e1b5",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this section, a new column was created called \"net_feelies\" (calculated by the authors as the total joy count minus the total despair count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sliced for easier calculations\n",
    "candy_reduced = candy.loc[:, 'Q6 | 100 Grand Bar':'Q6 | York Peppermint Patties']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986cc383",
   "metadata": {},
   "source": [
    "Next, I created two Series, one with JOY counts and one with DESPAIR counts to add to the `candy_reduced` data. `joy_count` lists total counts for JOY for each column, while `despair_count` that lists the total counts for DESPAIR for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bdcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "joy_count = pd.Series(candy_reduced[candy_reduced == 'JOY'].count())\n",
    "despair_count = pd.Series(candy_reduced[candy_reduced == 'DESPAIR'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose of the candy_reduced DataFrame\n",
    "candy_reduced_transpose = candy_reduced.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d76271",
   "metadata": {},
   "source": [
    "I added a new column called \"joy_count\" using the `joy_count` Series above and a new column called 'despair_count\" using the `despair_count` Series above to the `candy_reduced_transpose` DataFrame. I then added a new column to the `candy_reduced_transpose` DataFrame called \"net_feelies\" that takes the `joy_count` column and subtracts the `despair_count` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy_reduced_transpose['joy_count'] = joy_count\n",
    "candy_reduced_transpose['despair_count'] = despair_count\n",
    "candy_reduced_transpose['net_feelies'] = candy_reduced_transpose['joy_count'] - candy_reduced_transpose['despair_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797eb8c",
   "metadata": {},
   "source": [
    "This data was then sorted in descending order by `net feelies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy_net_sorted = candy_reduced_transpose.loc[:, 'joy_count':'net_feelies'].sort_values(\n",
    "    by = 'net_feelies', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0faa71",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "The next step is to get the `candy` DataFrame ready to run a machine learning algorthim to determine if we could predict a person's gender based on what candy they prefer. I turned all of the values into numeric values, using Pandas to perform these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of the DataFrame\n",
    "\n",
    "candy_encode = candy.copy()\n",
    "#candy_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586820c",
   "metadata": {},
   "source": [
    "First, I replaced any `Female` values with `0` and any `Male` values with `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map = {'Female' : 0, 'Male' : 1}\n",
    "\n",
    "candy_encode['Q2: GENDER'] = candy_encode['Q2: GENDER'].map(gender_map)\n",
    "\n",
    "#candy_encode.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92711e53",
   "metadata": {},
   "source": [
    "Then, I separateed the column that we want to predict (response) and the columns that will be used to make the predictions (features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy_response = pd.Series(candy_encode['Q2: GENDER'])\n",
    "\n",
    "encode_drop = ['Q2: GENDER', 'Q1: GOING OUT?', 'Q3: AGE', 'Q4: COUNTRY', 'Q10: DRESS', 'Q11: DAY', \n",
    "               'Q12: MEDIA [Daily Dish]', 'Q12: MEDIA [Science]', 'Q12: MEDIA [ESPN]', 'Q12: MEDIA [Yahoo]']\n",
    "candy_features = candy_encode.drop(encode_drop, axis = 1)\n",
    "\n",
    "candy_features = candy_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba76870",
   "metadata": {},
   "source": [
    "Finally, I used Pandas' `get_dummies()` to encode the `candy_features` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "candy_features_encoded = pd.get_dummies(candy_features, drop_first = True)\n",
    "candy_features_encoded = candy_features_encoded.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3851315",
   "metadata": {},
   "source": [
    "At this point, the data is now cleaned, encoded, and ready to be used to create a machine learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
